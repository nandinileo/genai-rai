**IDENTIFY**

Harms are unique to each scenarios. First task to identify the harms for your scenario. 
You can identify harms through an impact assessment, iterative red team testing, stress-testing, and analysis. Red teaming and stress-testing are approaches where a group of testers come together and intentionally probe a system to identify its limitations, risk surface, and vulnerabilities.

**RAI Impact Assessment:**
The link here directs you to the RAI Impact Assessment guide and templates. This template is really useful in the sense it will have series of questions for you to fill that will help you think about the harms and how to mitigate those. <a href="https://www.microsoft.com/en-us/ai/tools-practices" style="text-decoration: underline;">RAI Impact Assessment</a>
 
**Red Teaming:** The red teaming guidance will help you choose the appropriate team, guidance to the team, how to do red teaming etc. <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming" style="text-decoration: underline;">Red Teaming</a>

![image](https://github.com/user-attachments/assets/fc2a9cf5-8515-4dbf-91ae-4773921df18c)

This step will help us to identify high priority risks such riskful content based on the use case – harmful content includes hate, violence, sexual and self risk, Also identify for Jailbreaks, cross-prompt injection, ungroundedness, copyright, code risks – malicious or vulnerable code, advise in sensitive domains. As I mentioned before not all the risks will be applicable to all the scenarios. 
